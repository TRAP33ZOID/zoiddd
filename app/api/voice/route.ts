import { NextResponse } from "next/server";
import { speechToText, textToAudio } from "@/lib/voice";
import { retrieveContext } from "@/lib/rag";
import { ai, CHAT_MODEL } from "@/lib/gemini";
import { getSystemInstruction, isValidLanguage, getDefaultLanguage } from "@/lib/language";

export async function POST(req: Request) {
  const requestStartTime = Date.now();
  console.log("\n========================================");
  console.log("üé§ [VOICE API] New voice request received");
  console.log("========================================");
  
  try {
    // 1. Extract audio from request and get language parameter
    console.log("üì¶ [VOICE API] Parsing form data...");
    const formData = await req.formData();
    const audioBlob = formData.get("audio") as Blob;
    const languageParam = (formData.get("language") as string) || getDefaultLanguage();
    
    console.log(`  Language Parameter: ${languageParam}`);
    console.log(`  Audio Blob Type: ${audioBlob?.type}`);
    console.log(`  Audio Blob Size: ${audioBlob?.size} bytes`);
    
    // Validate language parameter
    const language = isValidLanguage(languageParam) ? languageParam : getDefaultLanguage();
    
    if (languageParam !== language) {
      console.log(`  ‚ö†Ô∏è WARNING: Invalid language '${languageParam}', using default '${language}'`);
    }

    if (!audioBlob) {
      console.error("  ‚ùå ERROR: Audio blob missing");
      return NextResponse.json(
        { error: "Audio blob is required" },
        { status: 400 }
      );
    }

    const audioBuffer = Buffer.from(await audioBlob.arrayBuffer());
    console.log(`  ‚úÖ Audio buffer created: ${audioBuffer.length} bytes`);

    // 2. Convert speech to text (STT)
    console.log("\nüé§ [VOICE API] Step 2: Starting STT...");
    const sttStartTime = Date.now();
    const userQuery = await speechToText(audioBuffer, "WEBM_OPUS", language);
    const sttDuration = Date.now() - sttStartTime;
    console.log(`  ‚úÖ STT completed in ${sttDuration}ms`);
    console.log(`  Transcribed query: "${userQuery}"`);

    // 3. Retrieve context using RAG
    console.log("\nüìö [VOICE API] Step 3: Retrieving context...");
    const ragStartTime = Date.now();
    const contextChunks = await retrieveContext(userQuery, language);
    const ragDuration = Date.now() - ragStartTime;
    console.log(`  ‚úÖ RAG completed in ${ragDuration}ms`);
    console.log(`  Context chunks retrieved: ${contextChunks.length}`);
    const context = contextChunks.join("\n---\n");

    // 4. Generate AI response using Gemini
    console.log("\nü§ñ [VOICE API] Step 4: Generating AI response...");
    const aiStartTime = Date.now();
    const baseSystemInstruction = getSystemInstruction(language);
    const systemInstruction = `${baseSystemInstruction}

CONTEXT:
---
${context}
---
`;

    const response = await ai.models.generateContent({
      model: CHAT_MODEL,
      contents: [{ role: "user", parts: [{ text: userQuery }] }],
      config: {
        systemInstruction: systemInstruction,
      },
    });

    const aiResponse = response.text || "Sorry, I could not generate a response.";
    const aiDuration = Date.now() - aiStartTime;
    console.log(`  ‚úÖ AI response generated in ${aiDuration}ms`);
    console.log(`  Response preview: "${aiResponse.substring(0, 100)}${aiResponse.length > 100 ? '...' : ''}"`);

    // 5. Convert AI response to speech (TTS)
    console.log("\nüîä [VOICE API] Step 5: Converting to speech...");
    const ttsStartTime = Date.now();
    const audioResponseBuffer = await textToAudio(aiResponse, language);
    const ttsDuration = Date.now() - ttsStartTime;
    console.log(`  ‚úÖ TTS completed in ${ttsDuration}ms`);

    // 6. Return both text and audio to client
    const totalDuration = Date.now() - requestStartTime;
    console.log("\n‚úÖ [VOICE API] Request completed successfully");
    console.log(`  Total Duration: ${totalDuration}ms`);
    console.log(`    - STT: ${sttDuration}ms`);
    console.log(`    - RAG: ${ragDuration}ms`);
    console.log(`    - AI: ${aiDuration}ms`);
    console.log(`    - TTS: ${ttsDuration}ms`);
    console.log("========================================\n");
    
    return NextResponse.json({
      transcript: userQuery,
      response: aiResponse,
      audioBuffer: audioResponseBuffer.toString("base64"),
      context: contextChunks,
      language: language,
      costData: {
        sttDuration: sttDuration,
        ttsCharacters: aiResponse.length,
        geminiMetadata: response.usageMetadata ? {
          promptTokenCount: response.usageMetadata.promptTokenCount || 0,
          candidatesTokenCount: response.usageMetadata.candidatesTokenCount || 0,
        } : null
      }
    });
  } catch (error: any) {
    const totalDuration = Date.now() - requestStartTime;
    console.error("\n‚ùå [VOICE API] Request FAILED");
    console.error(`  Total Duration: ${totalDuration}ms`);
    console.error("  Error Type:", error.constructor?.name || "Unknown");
    console.error("  Error Message:", error.message);
    console.error("  Error Stack:", error.stack);
    console.error("========================================\n");
    
    return NextResponse.json(
      {
        error: "Internal Server Error during voice processing",
        details: error.message,
        errorType: error.constructor?.name || "Unknown"
      },
      { status: 500 }
    );
  }
}